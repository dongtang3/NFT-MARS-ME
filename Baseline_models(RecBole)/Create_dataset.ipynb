{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create atomic files \n",
    "- User-item interactions\n",
    "    - Raw data (.csv) -> Atomic files (.inter)\n",
    "- Item features\n",
    "    - Raw data (.csv) -> Atomic files (.itememb)\n",
    "- dtype\n",
    "    - s: string\n",
    "    - d: digit\n",
    "    - f: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./dataset/transactions/azuki.csv',\n",
       " './dataset/transactions/bayc.csv',\n",
       " './dataset/transactions/coolcats.csv',\n",
       " './dataset/transactions/doodles.csv',\n",
       " './dataset/transactions/meebits.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw data .csv files \n",
    "files = glob.glob('./dataset/transactions/*.csv')\n",
    "files.sort()\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azuki', 'bayc', 'coolcats', 'doodles', 'meebits']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get names which is the one before '.csv'\n",
    "names = [os.path.basename(x).split('.')[0] for x in files]\n",
    "names.sort() \n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 33.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection:  azuki\n",
      "num of interactions:  15708\n",
      "\n",
      "collection:  bayc\n",
      "num of interactions:  13737\n",
      "\n",
      "collection:  coolcats\n",
      "num of interactions:  14890\n",
      "\n",
      "collection:  doodles\n",
      "num of interactions:  7250\n",
      "\n",
      "collection:  meebits\n",
      "num of interactions:  21104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for name, file in zip(tqdm(names), files):\n",
    "\n",
    "    df_azuki = pd.read_csv(file)\n",
    "    # df_azuki = df_azuki.drop_duplicates(subset=['Buyer', 'Token ID'], keep='first') # drop duplicated interactions\n",
    "    user = df_azuki['Buyer'].values\n",
    "    item = df_azuki['Token ID'].values\n",
    "    print('collection: ', name)\n",
    "    print('num of interactions: ', len(user))\n",
    "    print('')\n",
    "\n",
    "    save_path = './dataset/collections/' + name\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    f = open(save_path + f\"/{name}.inter\", 'w')\n",
    "    f.write(\"user_id:token\\titem_id:token\\n\")\n",
    "    for i in range(len(user)):\n",
    "        f.write(\"%s\\t%d\\n\"%(user[i], item[i]))\n",
    "    f.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .itememb\n",
    "img, txt, prices, txns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name:  azuki\n",
      "---  img\n",
      "before:  (10000, 1025)\n",
      "after:  (6659, 1025)\n",
      "---  txt\n",
      "before:  (10000, 1801)\n",
      "after:  (6659, 1801)\n",
      "---  prices\n",
      "before:  (8908, 65)\n",
      "after:  (6659, 65)\n",
      "---  txns\n",
      "before:  (8908, 65)\n",
      "after:  (6659, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:53<03:32, 53.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name:  bayc\n",
      "---  img\n",
      "before:  (9983, 1025)\n",
      "after:  (6726, 1025)\n",
      "---  txt\n",
      "before:  (10000, 1801)\n",
      "after:  (6726, 1801)\n",
      "---  prices\n",
      "before:  (8884, 65)\n",
      "after:  (6726, 65)\n",
      "---  txns\n",
      "before:  (8884, 65)\n",
      "after:  (6726, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:47<02:40, 53.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name:  coolcats\n",
      "---  img\n",
      "before:  (9952, 1025)\n",
      "after:  (6824, 1025)\n",
      "---  txt\n",
      "before:  (9941, 1501)\n",
      "after:  (6824, 1501)\n",
      "---  prices\n",
      "before:  (8384, 65)\n",
      "after:  (6824, 65)\n",
      "---  txns\n",
      "before:  (8384, 65)\n",
      "after:  (6824, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [02:35<01:42, 51.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name:  doodles\n",
      "---  img\n",
      "before:  (9999, 1025)\n",
      "after:  (4771, 1025)\n",
      "---  txt\n",
      "before:  (10000, 1501)\n",
      "after:  (4771, 1501)\n",
      "---  prices\n",
      "before:  (8285, 65)\n",
      "after:  (4771, 65)\n",
      "---  txns\n",
      "before:  (8285, 65)\n",
      "after:  (4771, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [03:11<00:45, 45.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name:  meebits\n",
      "---  img\n",
      "before:  (20000, 1025)\n",
      "after:  (6693, 1025)\n",
      "---  txt\n",
      "before:  (20000, 1801)\n",
      "after:  (6693, 1801)\n",
      "---  prices\n",
      "before:  (9682, 65)\n",
      "after:  (6693, 65)\n",
      "---  txns\n",
      "before:  (9682, 65)\n",
      "after:  (6693, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:06<00:00, 49.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(names):\n",
    "    print('Collection name: ', name)\n",
    "    for attribute in ['img', 'txt', 'prices', 'txns']:\n",
    "        print('--- ', attribute)\n",
    "\n",
    "        # Get raw data file\n",
    "        file = pd.read_csv(f'./dataset/features_item/{name}_{attribute}.csv')\n",
    "        print('before: ', file.shape)\n",
    "        \n",
    "        # interaction에 등장하는 아이템만 남기기\n",
    "        inter = pd.read_csv(f'./dataset/collections/{name}/{name}.inter', sep='\\t')\n",
    "        token_ids = inter['item_id:token'].unique()\n",
    "        file = file[file['token_id'].isin(token_ids)].reset_index(drop=True)\n",
    "        print('after: ', file.shape)\n",
    "\n",
    "        # save flie\n",
    "        save_path = './dataset/collections/' + name\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        f = open(save_path + f\"/{name}.itememb_{attribute}\", 'w')\n",
    "        f.write(f\"iid_{attribute}:token\" + '\\t' + f'item_emb_{attribute}:float_seq' + '\\n')\n",
    "        for i in range(len(file)):\n",
    "            # get token_id\n",
    "            token_id = file['token_id'][i]\n",
    "            # get the rest of the features\n",
    "            features = file.iloc[i, 1:] # Series\n",
    "            # write\n",
    "            f.write(str(token_id) + '\\t')\n",
    "            for j in range(len(features)):\n",
    "                f.write(f\"{features[j].astype(np.float32)}\") \n",
    "                # if it is not the last iteration\n",
    "                if j != len(features) - 1:\n",
    "                    f.write(' ')\n",
    "            f.write('\\n')    \n",
    "\n",
    "        f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_path = './dataset/collections/'\n",
    "features_path = './dataset/features_item/'\n",
    "collection_names = ['azuki', 'bayc', 'coolcats', 'doodles', 'meebits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  (8908, 2827)\n",
      "after:  (6659, 2827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6659/6659 [00:17<00:00, 383.73it/s]\n",
      " 20%|██        | 1/5 [00:20<01:20, 20.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  (8867, 2827)\n",
      "after:  (6726, 2827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6726/6726 [00:17<00:00, 385.55it/s]\n",
      " 40%|████      | 2/5 [00:40<01:00, 20.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  (8373, 2527)\n",
      "after:  (6824, 2527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6824/6824 [00:17<00:00, 393.33it/s]\n",
      " 60%|██████    | 3/5 [00:59<00:39, 19.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  (8284, 2527)\n",
      "after:  (4771, 2527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4771/4771 [00:11<00:00, 426.74it/s]\n",
      " 80%|████████  | 4/5 [01:13<00:17, 17.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  (9682, 2827)\n",
      "after:  (6693, 2827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6693/6693 [00:17<00:00, 381.79it/s]\n",
      "100%|██████████| 5/5 [01:35<00:00, 19.13s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "combined\n",
    "\"\"\"\n",
    "for COLLECTION in tqdm(collection_names):\n",
    "\n",
    "    # read data\n",
    "    image = pd.read_csv(os.path.join(features_path, f'{COLLECTION}_img.csv'), encoding='utf-8')\n",
    "    text = pd.read_csv(os.path.join(features_path, f'{COLLECTION}_txt.csv'), encoding='utf-8')\n",
    "    price = pd.read_csv(os.path.join(features_path, f'{COLLECTION}_prices.csv'), encoding='utf-8')\n",
    "    txns = pd.read_csv(os.path.join(features_path, f'{COLLECTION}_txns.csv'), encoding='utf-8')\n",
    "    # change first column name to 'token_id'\n",
    "    image.columns = ['token_id'] + list(image.columns[1:])\n",
    "    text.columns = ['token_id'] + list(text.columns[1:])\n",
    "    price.columns = ['token_id'] + list(price.columns[1:])\n",
    "    txns.columns = ['token_id'] + list(txns.columns[1:])\n",
    "    # drop columns after 3rd\n",
    "    price = price.iloc[:, :2]\n",
    "    txns = txns.iloc[:, :2]\n",
    "    # combine image, text, price, txns into one dataframe \n",
    "    combined = pd.merge(image, text, on='token_id')\n",
    "    combined = pd.merge(combined, price, on='token_id')\n",
    "    combined = pd.merge(combined, txns, on='token_id')\n",
    "    print('before: ', combined.shape)\n",
    "\n",
    "    # interaction에 등장하는 아이템만 남기기\n",
    "    inter = pd.read_csv(f'./dataset/collections/{COLLECTION}/{COLLECTION}.inter', sep='\\t')\n",
    "    token_ids = inter['item_id:token'].unique()\n",
    "    combined = combined[combined['token_id'].isin(token_ids)].reset_index(drop=True)\n",
    "    print('after: ', combined.shape)\n",
    "\n",
    "    # .item 저장하기\n",
    "    f = open(os.path.join(collection_path, f'{COLLECTION}/{COLLECTION}.item'), 'w')\n",
    "    f.write(\"item_id:token\" + '\\t' + 'img:float_seq' + '\\t' + 'txt:float_seq' + '\\t' + 'price:float' + '\\t' +'txn:float' + '\\n')\n",
    "\n",
    "    for idx, row in tqdm(combined.iterrows(), total = len(combined)):\n",
    "        \n",
    "        image_feat = combined.iloc[idx, 1:1025] # Series\n",
    "        text_feat = combined.iloc[idx, 1025:-2] # Series\n",
    "        price_feat = combined.iloc[idx, -2] # float\n",
    "        transaction_feat = combined.iloc[idx, -1] # float\n",
    "\n",
    "        # 1. token_id\n",
    "        f.write(\"%d\"%(row['token_id']))\n",
    "        f.write('\\t')\n",
    "        \n",
    "        # 2. image_feat\n",
    "        for idx, i in enumerate(image_feat):\n",
    "            f.write(\"%f\"%(i))\n",
    "            if idx != len(image_feat)-1:\n",
    "                f.write(' ')\n",
    "        f.write('\\t')\n",
    "\n",
    "        # 3. text_feat\n",
    "        for idx, i in enumerate(text_feat):\n",
    "            f.write(\"%f\"%(i))\n",
    "            if idx != len(text_feat)-1:\n",
    "                f.write(' ')\n",
    "        f.write('\\t')\n",
    "\n",
    "        # 4. price_feat\n",
    "        f.write(\"%f\"%(price_feat))\n",
    "        f.write('\\t')\n",
    "        \n",
    "        # 5. transaction_feat\n",
    "        f.write(\"%f\"%(transaction_feat))\n",
    "        \n",
    "        f.write('\\n')\n",
    "\n",
    "    f.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_path = './dataset/collections/'\n",
    "features_path = './dataset/features_user/'\n",
    "collection_names = ['azuki', 'bayc', 'coolcats', 'doodles', 'meebits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after:  (1647, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1647/1647 [00:00<00:00, 46640.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after:  (1230, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1230/1230 [00:00<00:00, 45337.85it/s]\n",
      " 40%|████      | 2/5 [00:00<00:00, 13.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after:  (1357, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1357/1357 [00:00<00:00, 44341.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after:  (804, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 804/804 [00:00<00:00, 44296.13it/s]\n",
      " 80%|████████  | 4/5 [00:00<00:00, 16.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after:  (1184, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1184/1184 [00:00<00:00, 44231.97it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.16it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "combined\n",
    "\"\"\"\n",
    "for COLLECTION in tqdm(collection_names):\n",
    "\n",
    "    # read data\n",
    "    combined = pd.read_csv(os.path.join(features_path, 'user_features.csv'), encoding='utf-8')\n",
    "\n",
    "    # interaction에 등장하는 유저만 남기기\n",
    "    inter = pd.read_csv(f'./dataset/collections/{COLLECTION}/{COLLECTION}.inter', sep='\\t')\n",
    "    token_ids = inter['user_id:token'].unique()\n",
    "    combined = combined[combined['Buyer'].isin(token_ids)].reset_index(drop=True)\n",
    "    print('after: ', combined.shape)\n",
    "\n",
    "    # .user 저장하기\n",
    "    f = open(os.path.join(collection_path, f'{COLLECTION}/{COLLECTION}.user'), 'w')\n",
    "    f.write(\"user_id:token\" + '\\t' + 'num_txn:token' + '\\t' + 'avg_price:float' + '\\t' + 'hold_period:float' + '\\n')\n",
    "\n",
    "    for idx, row in tqdm(combined.iterrows(), total = len(combined)):\n",
    "\n",
    "        # 1. token_id\n",
    "        f.write(\"%s\"%(row['Buyer']))\n",
    "        f.write('\\t')\n",
    "        \n",
    "        # 2. num_txn\n",
    "        f.write(\"%d\"%(row['# of transactions']))\n",
    "        f.write('\\t')        \n",
    "        # 3. avg_price\n",
    "        f.write(\"%f\"%(row['Avg transaction price']))\n",
    "        f.write('\\t')        \n",
    "        # 4. hold_period\n",
    "        f.write(\"%f\"%(row['holding period']))\n",
    "        \n",
    "        f.write('\\n')\n",
    "\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecBole_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "991fe3f9de00c9a422a5f66b8cc7243158afe66a42c9654a2fcf9d740859f175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
